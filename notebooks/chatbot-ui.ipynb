{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade langchain langchain-community langchain-chroma langchain-openai tenacity==8.5.0 python-dotenv gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "file_path='./idea-2024-new.json'\n",
    "data = json.loads(Path(file_path).read_text(encoding='utf-8'))\n",
    "loader = JSONLoader(\n",
    "         file_path=file_path,\n",
    "         jq_schema=\".[].summary\",\n",
    "         text_content=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://1185ba6fddecb0e6b6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1185ba6fddecb0e6b6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7861 <> https://1185ba6fddecb0e6b6.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "green_theme = gr.themes.Base(\n",
    "    primary_hue=gr.themes.Color(\n",
    "        c50=\"#00A168\",     \n",
    "        c100=\"#57B485\",   \n",
    "        c200=\"#D7ECE0\",   \n",
    "        c300=\"#FFFFFF\",   \n",
    "        c400=\"#EAE9E9\",   \n",
    "        c500=\"#000000\",   \n",
    "        c600=\"#3A905E\",   \n",
    "        c700=\"#2A774A\",   \n",
    "        c800=\"#1A5E36\",   \n",
    "        c900=\"#0A4512\",   \n",
    "        c950=\"#052A08\"    \n",
    "    ),\n",
    "    font=[gr.themes.GoogleFont('Space Grotesk'), 'ui-sans-serif', 'system-ui', 'sans-serif']\n",
    ").set(\n",
    "    body_background_fill='#00A168',               \n",
    "    body_text_color='#000000',                    \n",
    "    background_fill_primary='#FFFFFF',            \n",
    "    background_fill_secondary='#FFFFFF',          \n",
    "    border_color_accent='#57B485',                \n",
    "    border_color_accent_subdued='#EAE9E9',        \n",
    "    color_accent='#57B485',                       \n",
    "    color_accent_soft='#D7ECE0',                  \n",
    "    checkbox_background_color='#FFFFFF',          \n",
    "    button_primary_background_fill='#57B485',     \n",
    "    button_primary_background_fill_hover='#3A905E', \n",
    "    button_secondary_background_fill='#D7ECE0',   \n",
    "    button_secondary_text_color='#000000'         \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# integrate gradio with RAG logic\n",
    "def message_and_history(message, history):\n",
    "    history = history or [{\"role\": \"assistant\", \"content\": \"<b>LA2050 Navigator:</b><br> Welcome to the LA2050 ideas hub! How can I help you today?\"}]    \n",
    "    history.append({\"role\": \"user\", \"content\": message.get(\"text\", \"\")})\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    user_input = message.get(\"text\", \"\")\n",
    "    if not user_input:\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"<b>LA2050 Navigator:</b><br> Please enter a valid message.\"})\n",
    "        yield history, history\n",
    "        return\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": user_input})\n",
    "    answer = response[\"answer\"]\n",
    "\n",
    "    dynamic_message = {\"role\": \"assistant\", \"content\": \"<b>LA2050 Navigator:</b><br> \"}\n",
    "    history.append(dynamic_message)\n",
    "\n",
    "    for character in answer:\n",
    "        dynamic_message[\"content\"] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history, history\n",
    "\n",
    "    history[-1][\"content\"] = f\"<b>LA2050 Navigator:</b><br> {answer}\"\n",
    "    yield history, history\n",
    "\n",
    "\n",
    "# set to light mode\n",
    "js_func = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "\n",
    "    if (url.searchParams.get('__theme') !== 'light') {\n",
    "        url.searchParams.set('__theme', 'light');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "css = \"\"\"\"\"\n",
    "    .chat-header {\n",
    "    text-color: #FFFFFF;\n",
    "    text-align: center;\n",
    "    }\n",
    "    .gradio-container .prose .chat-header h1 {\n",
    "    color: #FFFFFF;\n",
    "    text-align: center; \n",
    "    }\n",
    "   \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# setup gradio interface\n",
    "with gr.Blocks(theme=green_theme, js=js_func, css=css) as block:\n",
    "\n",
    "    gr.HTML('<div class=\"chat-header\"><h1>LA2050 Navigator</h1></div>')\n",
    "    \n",
    "    chatbot = gr.Chatbot( value= \n",
    "                         [{\"role\": \"assistant\", \"content\": \"<b>LA2050 Navigator:</b><br> Welcome to the LA2050 ideas hub! How can I help you today?\"}],\n",
    "                            type=\"messages\",\n",
    "                            bubble_full_width=False)\n",
    "    \n",
    "    state = gr.State([])\n",
    "\n",
    "    message = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"Type a message\", \n",
    "        label=\"\", \n",
    "        elem_classes=\"custom-textbox\", \n",
    "        scale=3,\n",
    "        show_label=False)\n",
    "    \n",
    "    message.submit( \n",
    "        message_and_history, \n",
    "        inputs=[message, state],\n",
    "        outputs=[chatbot, state]\n",
    "    ).then(\n",
    "        lambda: \"\", inputs=[], outputs=message  \n",
    "    )\n",
    "\n",
    "block.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
